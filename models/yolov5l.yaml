# YOLOv5 🚀 by Ultralytics, AGPL-3.0 license

# Parameters
nc: 80 # number of classes
depth_multiple: 1.0 # model depth multiple
width_multiple: 1.0 # layer channel multiple

#在给定的配置中：
#[10, 13, 16, 30, 33, 23] 对应于 P3/8 层级的锚点框，它们通常用于检测小尺寸的对象。
#[30, 61, 62, 45, 59, 119] 对应于 P4/16 层级的锚点框，用于检测中等尺寸的对象。
#[116, 90, 156, 198, 373, 326] 对应于 P5/32 层级的锚点框，用于检测大尺寸的对象。
#具体来说，P3/8、P4/16 和 P5/32 代表不同的特征图层级，数字指的是原始图像大小的降采样倍数。例如，P3/8 的特征图是原始输入图像大小的 1/8。
#一对数字 [宽度, 高度] 是基于原始输入图像尺寸的一部分。例如，如果原始图像尺寸是 640x640 像素，一个 [10, 13] 的锚点框将有 10x13 像素大小的区域。

#让我们澄清一下关于 YOLO 模型中 anchor boxes 的工作原理和它们如何与特征图关联的理解。
#确实，anchor boxes 是作用在对应层级的特征图上的。在 YOLO 中，每个特征图的单元格都负责预测该单元格感受野范围内的对象。因此，虽然特征图的尺寸可能相对较小（比如 20x20），但每个单元格会对应原始输入图像中的一个更大区域。
#这里的关键点是：anchor boxes 的尺寸是根据原始输入图像的尺寸来预先设定的，并且这些尺寸是在训练期间通过分析训练数据集中的实际边界框尺寸来确定的。然后，这些尺寸会被用于初始化在特征图上每个单元格的边界框预测。虽然 anchor 尺寸是针对原始图像尺寸设定的，但实际操作时它们会被映射到特征图的尺度上。
#每个 anchor box 在特征图单元格中的具体角色是提供一个参考框架，以便模型能够通过调整这些参考框架的位置（中心 x, y 坐标）、尺寸（宽度和高度）和对象存在的概率来预测真实的边界框。模型实际上是在学习如何根据每个特征图单元格的感受野调整这些 anchor boxes。
#因此，即使在小尺寸的特征图上，比如 20x20，也可以有较大尺寸的 anchor boxes。这些大尺寸的 anchor boxes 对应于原始图像上的大区域，因此它们在检测大物体时非常有用，因为大物体会覆盖更多的单元格。这就是为什么在 P5 这样的较低分辨率的特征图上，我们会有 [116, 90] 这样尺寸的 anchors，尽管这些数字超出了特征图单元格的数量。
#总结来说，anchor boxes 的设计允许 YOLO 模型在不同尺度的特征图上预测不同尺寸的物体，其尺寸设置基于原始图像维度，并且在预测时会映射到特征图上对应的尺度。

#实际例子
#想象一下，你有一张很大的画布，上面有很多小的点（小物体）和几个大的圆圈（大物体）。如果你用一个小网格去覆盖这张画布，你可以精确地圈出小点，但对于大圆圈，你需要多个小网格来覆盖。现在如果你改用一个大网格，小点可能就不那么明显了，但你可以很容易地用一个大网格单元来圈出一个大圆圈。
#同理，在YOLO模型中，步长为8的高分辨率特征图（P3/8）能够用小的anchors来精确地定位小物体，而步长为32的低分辨率特征图（P5/32）则需要大的anchors来有效地覆盖和检测图像中的大物体。

anchors:
  - [10, 13, 16, 30, 33, 23] # P3/8
  - [30, 61, 62, 45, 59, 119] # P4/16
  - [116, 90, 156, 198, 373, 326] # P5/32

# YOLOv5 v6.0 backbone
backbone:
  # [from, number, module, args]
  [
    [-1, 1, Conv, [64, 6, 2, 2]], # 0-P1/2
    [-1, 1, Conv, [128, 3, 2]], # 1-P2/4
    [-1, 3, C3, [128]],
    [-1, 1, Conv, [256, 3, 2]], # 3-P3/8
    [-1, 6, C3, [256]],
    [-1, 1, Conv, [512, 3, 2]], # 5-P4/16
    [-1, 9, C3, [512]],
    [-1, 1, Conv, [1024, 3, 2]], # 7-P5/32
    [-1, 3, C3, [1024]],
    [-1, 1, SPPF, [1024, 5]], # 9
  ]

# YOLOv5 v6.0 head
head: [
    [-1, 1, Conv, [512, 1, 1]],
    [-1, 1, nn.Upsample, [None, 2, "nearest"]],
    [[-1, 6], 1, Concat, [1]], # cat backbone P4
    [-1, 3, C3, [512, False]], # 13

    [-1, 1, Conv, [256, 1, 1]],
    [-1, 1, nn.Upsample, [None, 2, "nearest"]],
    [[-1, 4], 1, Concat, [1]], # cat backbone P3
    [-1, 3, C3, [256, False]], # 17 (P3/8-small)

    [-1, 1, Conv, [256, 3, 2]],
    [[-1, 14], 1, Concat, [1]], # cat head P4
    [-1, 3, C3, [512, False]], # 20 (P4/16-medium)

    [-1, 1, Conv, [512, 3, 2]],
    [[-1, 10], 1, Concat, [1]], # cat head P5
    [-1, 3, C3, [1024, False]], # 23 (P5/32-large)

    [[17, 20, 23], 1, Detect, [nc, anchors]], # Detect(P3, P4, P5)
  ]
